{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GitStractor Repository Analysis Data Extraction Tool\n",
    "\n",
    "Created by [Matt Eland](https://MattEland.dev)\n",
    "\n",
    "This notebook helps you load historical git and file statistics data from any local git repository"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-requisites\n",
    "The code assumes you have the following libraries installed:\n",
    "\n",
    "- pydriller\n",
    "- pandas"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup: Output file paths and repository"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This repository path should be a LOCAL path already cloned on disk\n",
    "repository_path = 'C:\\\\Dev\\\\GitStractor'\n",
    "repository_branch = 'main'\n",
    "\n",
    "# Declare our paths of interest\n",
    "commit_data_path = 'Commits.csv'\n",
    "file_commit_data_path = 'FileCommits.csv'\n",
    "file_size_data_path = 'FileSizes.csv'\n",
    "file_data_path = 'FileData.csv'\n",
    "author_data_path = 'Authors.csv'\n",
    "merged_data_path = 'MergedData.csv'\n",
    "merged_file_data_path = 'MergedFileData.csv'\n",
    "\n",
    "# Multi-threading is supported\n",
    "num_threads = 16"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configure Authors\n",
    "\n",
    "Sometimes people commit with different names or E-Mail addresses and we want to configure aliases. Additionally, the final project's rubric required a map view. The best way of doing that with this dataset is to allow the user to specify the country and city of each author."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "author_info = [\n",
    "    { \n",
    "        'name': 'Matt Eland', \n",
    "        'email': 'matt.eland@gmail.com', \n",
    "        'aliases': ['matt@mattondatascience.com']\n",
    "    },\n",
    "]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Pulling Commit Data\n",
    "\n",
    "We will use the PyDriller library to pull commit data from a local git repository and build a list of commits.\n",
    "\n",
    "This process can take a very long time depending on the size of the repository (0.25 - 1 seconds *per commit*, depending on your machine and the `num_threads`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Analyzing Git Repository at C:\\Dev\\GitStractor\n",
      "Fetching commits. This can take a long time...\n",
      "Read 2 commits and 11 file commits\n",
      "Saved to Commits.csv\n",
      "Saved to FileCommits.csv\n",
      "Repository Data Pulled Successfully\n"
     ]
    }
   ],
   "source": [
    "from Scripts import GitAnalyzer \n",
    "\n",
    "# This will pull all repository data and write to Commits.csv and FileCommits.csv\n",
    "GitAnalyzer.analyze_repository(repository_path, \n",
    "                               num_threads=num_threads,\n",
    "                               author_info=author_info,\n",
    "                               commits_file_path=commit_data_path,\n",
    "                               file_commits_file_path=file_commit_data_path,\n",
    "                               branch=repository_branch)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Building File Size Information\n",
    "\n",
    "Next we'll walk the directory looking at source files and build out a CSV file with all file data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading file metrics from C:\\Dev\\GitStractor\n",
      "5 source files read from C:\\Dev\\GitStractor\n",
      "File size information saved to FileSizes.csv\n"
     ]
    }
   ],
   "source": [
    "from Scripts import FileAnalyzer\n",
    "\n",
    "FileAnalyzer.build_file_sizes(repository_path, file_size_data_path)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Data Aggregation\n",
    "\n",
    "Next, we're going to unify all the data together into a MergedFileData.csv and FileData.csv files that let us do more in-depth analysis without having to manage the joins downstream\n",
    "\n",
    "**NOTE:** Nuances of commit history is currently lost for files that have been renamed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading file commit data from FileCommits.csv\n",
      "Loading file size data from FileSizes.csv\n",
      "Merged file data created in MergedFileData.csv\n",
      "Aggregating file data\n",
      "Writing file data to FileData.csv\n",
      "File generation completed\n"
     ]
    }
   ],
   "source": [
    "from Scripts import GitDataMerger\n",
    "\n",
    "GitDataMerger.generate_merged_file_data(file_commit_data_path, \n",
    "                                        file_size_data_path, \n",
    "                                        merged_file_data_path,\n",
    "                                        file_data_path)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ready for Analysis\n",
    "Next up is the actual data analysis workflow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Analysis complete. Data files are ready for import\n"
     ]
    }
   ],
   "source": [
    "print('Analysis complete. Data files are ready for import')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "a4868653bb6f8972e87e4c446ab8a445a15b25dedb8594cc74c480f8152ea86a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
